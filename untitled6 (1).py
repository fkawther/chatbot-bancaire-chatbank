# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13NCVBKre0CXpjmHwmYnfCUSbS3vSc2YX
"""

# === 1. Importations ===
import pandas as pd
import re
import string
from sentence_transformers import SentenceTransformer, util
import nltk
from nltk.corpus import stopwords
!pip install gradio


nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

#from google.colab import files
#uploaded = files.upload()

list(uploaded.keys())

#import io
#df = pd.read_csv(io.BytesIO(uploaded['Dataset_Banking_chatbot_eng.csv']))
#queries = df['user_query'].tolist()
#responses = df['bot_response'].tolist()

# === 3. Nettoyage du texte ===
def clean_text(text):
    text = text.lower()  # tout en minuscule
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)  # liens
    text = re.sub(r'\@w+|\#','', text)  # mentions / hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # garder que lettres et espaces
    text = text.translate(str.maketrans('', '', string.punctuation))  # enlever ponctuation
    text = " ".join([word for word in text.split() if word not in stop_words])  # enlever stopwords
    return text.strip()

cleaned_queries = [clean_text(q) for q in queries]

# === 4. Encodage avec modèle de sentence transformer ===
model = SentenceTransformer('all-MiniLM-L6-v2')
query_embeddings = model.encode(cleaned_queries, convert_to_tensor=True)



from sentence_transformers import SentenceTransformer

# 🔄 Remplacement des anciens modèles par des multilingues plus adaptés
models = {
    "MiniLM-Multilingual": SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2"),
    "DistilUSE-Multilingual": SentenceTransformer("distiluse-base-multilingual-cased-v2"),
    "LaBSE": SentenceTransformer("sentence-transformers/LaBSE")
}

# === 5. Fonction de réponse ===
def get_response(user_input, top_k=1):
    cleaned_input = clean_text(user_input)
    user_embedding = model.encode(cleaned_input, convert_to_tensor=True)
    similarities = util.cos_sim(user_embedding, query_embeddings)[0]

    top_idx = similarities.topk(k=top_k).indices.tolist()

    results = []
    for idx in top_idx:
        results.append({
            'matched_query': queries[idx],
            'response': responses[idx],
            'similarity': round(similarities[idx].item(), 3)
        })

    return results

# === 6. Exemple d'utilisation ===
user_question = "Can I create a new account online?"
results = get_response(user_question, top_k=3)

print(f"User: {user_question}")
for i, r in enumerate(results):
    print(f"\nMatch {i+1}:")
    print(f"Query: {r['matched_query']}")
    print(f"Similarity: {r['similarity']}")
    print(f"Bot: {r['response']}")

# === Ajout à ton script existant ===
import matplotlib.pyplot as plt

# === Fonction d’évaluation top-k ===
def evaluate_retrieval_accuracy(k_values=[1, 3]):
    topk_correct = {k: 0 for k in k_values}
    total = len(df)

    for i in range(total):
        expected_query = queries[i]
        test_input = queries[i]  # on suppose qu'on doit retrouver la même question
        results = get_response(test_input, top_k=max(k_values))

        for k in k_values:
            top_k_matches = [r['matched_query'] for r in results[:k]]
            if expected_query in top_k_matches:
                topk_correct[k] += 1

    accuracies = {k: topk_correct[k] / total for k in k_values}
    return accuracies

# === Calcul et affichage ===
accuracy_scores = evaluate_retrieval_accuracy()
print("\n--- Accuracy Scores ---")
for k, acc in accuracy_scores.items():
    print(f"Top-{k} Accuracy: {acc:.2%}")

# === Graphique des scores ===
plt.figure(figsize=(6, 4))
plt.bar([f"Top-{k}" for k in accuracy_scores.keys()], accuracy_scores.values(), color='skyblue')
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Retrieval Accuracy (Top-K)")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()

# === 1. Métrique : Mean Reciprocal Rank (MRR) ===
def evaluate_mrr():
    total_rr = 0.0
    total = len(df)

    for i in range(total):
        expected_query = queries[i]
        results = get_response(expected_query, top_k=10)  # cherche dans top 10

        for rank, r in enumerate(results, start=1):
            if r['matched_query'] == expected_query:
                total_rr += 1.0 / rank
                break  # on prend la première occurrence correcte

    mrr = total_rr / total
    return mrr

# === 2. Confusion simplifiée (Top-1 erreurs uniquement) ===
def show_confusion_errors(max_errors=10):
    errors = []
    for i in range(len(df)):
        expected_query = queries[i]
        result = get_response(expected_query, top_k=1)[0]
        if result['matched_query'] != expected_query:
            errors.append((expected_query, result['matched_query'], result['similarity']))
        if len(errors) >= max_errors:
            break
    return errors

# === MRR ===
mrr_score = evaluate_mrr()
print(f"\nMean Reciprocal Rank (MRR): {mrr_score:.3f}")

# === Erreurs Top-1 ===
confusion_errors = show_confusion_errors()

print("\n--- Top-1 Confusion Examples ---")
for i, (expected, predicted, sim) in enumerate(confusion_errors, 1):
    print(f"\nErreur {i}:")
    print(f"Expected: {expected}")
    print(f"Predicted: {predicted}")
    print(f"Similarity: {sim:.3f}")

#!pip install langdetect

from langdetect import detect

# ✅ Reprendre les modèles de traduction (assure-toi qu'ils sont déjà chargés)
# model_en_fr, tokenizer_en_fr
# model_en_ar, tokenizer_en_ar

def translate_response(text, target_lang):
    if target_lang == "fr":
        return translate(text, model_en_fr, tokenizer_en_fr)
    elif target_lang == "ar":
        return translate(text, model_en_ar, tokenizer_en_ar)
    else:
        return text  # pas besoin de traduire si c'est déjà en anglais

def test_chatbot_manuel(question, model_name="MiniLM-Multilingual"):
    model = models[model_name]
    input_emb = model.encode(question, convert_to_tensor=True)
    corpus = df['user_query'].tolist()
    embeddings = model.encode(corpus, convert_to_tensor=True)
    sims = util.cos_sim(input_emb, embeddings)[0]
    idx = int(sims.argmax())
    response = df.iloc[idx]['bot_response']

    # 🔍 Détecter la langue de la question
    try:
        lang = detect(question)
    except:
        lang = "en"

    # 🌍 Traduire si nécessaire
    translated_response = translate_response(response, lang)

    print(f"📥 Question détectée ({lang}): {question}")
    print(f"🤖 Réponse dans la même langue : {translated_response}")

def translate(text, model, tokenizer):
    batch = tokenizer([text], return_tensors='pt', padding=True, truncation=True)
    gen = model.generate(**batch)
    return tokenizer.decode(gen[0], skip_special_tokens=True)

from transformers import MarianMTModel, MarianTokenizer

# Traduction EN ➤ FR
model_en_fr = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-fr")
tokenizer_en_fr = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-fr")

# Traduction EN ➤ AR
model_en_ar = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
tokenizer_en_ar = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ar")

test_chatbot_manuel("comment consulter mon solde ?", model_name="MiniLM-Multilingual")
test_chatbot_manuel("كيف أتحقق من رصيدي؟", model_name="MiniLM-Multilingual")

import gradio as gr

# Simuler les modèles sans les charger
model_choices = ["MiniLM-Multilingual", "DistilUSE-Multilingual", "LaBSE"]

# Exemple de fonction de réponse simple
def test_chatbot_manuel(question, model_name):
    return f"Réponse simulée avec le modèle : {model_name}"

# Interface Gradio
def interface_fn(question, model_choice):
    return test_chatbot_manuel(question, model_choice)

gr.Interface(
    fn=interface_fn,
    inputs=[
        gr.Textbox(label="Votre question"),
        gr.Dropdown(choices=model_choices, label="Modèle")
    ],
    outputs="text",
    title="Chatbot bancaire multilingue",
    description="Posez une question en 🇫🇷 Français, 🇬🇧 Anglais ou 🇹🇳 Arabe"
).launch()

from sentence_transformers import SentenceTransformer, util

models = {
    "MiniLM-Multilingual": SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2"),
    "DistilUSE-Multilingual": SentenceTransformer("distiluse-base-multilingual-cased-v2"),
    "LaBSE": SentenceTransformer("sentence-transformers/LaBSE")
}

def test_chatbot_manuel(question, model_name):
    model = models[model_name]
    emb_question = model.encode(question, convert_to_tensor=True)
    emb_dataset = model.encode(df['user_query'].tolist(), convert_to_tensor=True)
    sims = util.cos_sim(emb_question, emb_dataset)[0]
    idx_max = sims.argmax().item()
    return df.iloc[idx_max]['bot_response']



import gradio as gr

def interface_fn(question, model_choice):
    return test_chatbot_manuel(question, model_choice)

gr.Interface(
    fn=interface_fn,
    inputs=[
        gr.Textbox(label="Votre question"),
        gr.Dropdown(choices=list(models.keys()), label="Modèle")
    ],
    outputs="text",
    title="Chatbot bancaire multilingue",
    description="Posez une question en 🇫🇷 Français, 🇬🇧 Anglais ou 🇹🇳 Arabe"
).launch()

import os
import pandas as pd

if not os.path.exists("Dataset_Banking_chatbot_eng.csv"):
    raise FileNotFoundError("Le fichier 'Dataset_Banking_chatbot_eng.csv' est introuvable.")

df = pd.read_csv("Dataset_Banking_chatbot_eng.csv")

print(df.columns)

def test_chatbot_manuel(user_query, model_name="MiniLM-Multilingual"):
    from sentence_transformers import util

    # Prétraitement
    user_query = user_query.strip().lower()
    model = models[model_name]
    df_filtered = df[df['user_query'].str.lower() != user_query]

    if df_filtered.empty:
        return "Pas de réponse disponible."

    emb_user = model.encode(user_query, convert_to_tensor=True)
    emb_corpus = model.encode(df_filtered['user_query'].tolist(), convert_to_tensor=True)
    sims = util.cos_sim(emb_user, emb_corpus)[0]
    idx_max = int(sims.argmax())

    # 👇 La ligne qui déclenchait l'erreur est maintenant bien à l’intérieur
    return df_filtered.iloc[idx_max]['bot_response']

from langdetect import detect

def translate(text, model, tokenizer):
    batch = tokenizer([text], return_tensors='pt', padding=True, truncation=True)
    gen = model.generate(**batch)
    return tokenizer.decode(gen[0], skip_special_tokens=True)

def translate_response(text, target_lang):
    if target_lang == "fr":
        return translate(text, model_en_fr, tokenizer_en_fr)
    elif target_lang == "ar":
        return translate(text, model_en_ar, tokenizer_en_ar)
    else:
        return text

import gradio as gr

def interface_fn(question, model_choice):
    return test_chatbot_manuel(question, model_choice)